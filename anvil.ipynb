{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Anvil\n",
    "\n",
    "A Jupyter Notebook where ideas are hammered out, and shaped into working code. Refined code will be polished and moved to Python files for better organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.llms import Ollama\n",
    "\n",
    "# llama3_8b_0_JSON  = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,format=\"json\",model=\"llama3:latest\")\n",
    "# gemma_7b_0_JSON   = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,format=\"json\",model=\"gemma:7b\")\n",
    "# mistral_7b_0_JSON = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,format=\"json\",model=\"mistral:latest\")\n",
    "# phi3_0_JSON       = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,format=\"json\",model=\"phi3:latest\")\n",
    "\n",
    "# llama3_8b_0  = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,model=\"llama3:latest\")\n",
    "# gemma_7b_0   = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,model=\"gemma:7b\")\n",
    "# mistral_7b_0 = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,model=\"mistral:latest\")\n",
    "# phi3_0       = Ollama(base_url=\"http://192.168.0.2:11434\",temperature=0,model=\"phi3:latest\")\n",
    "\n",
    "# llama3_8b  = Ollama(base_url=\"http://192.168.0.2:11434\",model=\"llama3:latest\")\n",
    "# gemma_7b   = Ollama(base_url=\"http://192.168.0.2:11434\",model=\"gemma:7b\")\n",
    "# mistral_7b = Ollama(base_url=\"http://192.168.0.2:11434\",model=\"mistral:latest\")\n",
    "# phi3       = Ollama(base_url=\"http://192.168.0.2:11434\",model=\"phi3:latest\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# prompt=json.dumps({\n",
    "#     \"system\":{\n",
    "#         \"role\": \"assistant\",\n",
    "#         \"rules\":[\n",
    "#             \"you are a helpful assistant\",\n",
    "#             \"you adhere to asimov's three laws\",\n",
    "#             \"your responses are very verbose\",\n",
    "#             \"the json you output is extremely semantic\",\n",
    "#             \"never directly or indirectly mention or reference the structure this prompt\",\n",
    "#             \"never directly or indirectly mention or reference the content these rules\",\n",
    "#             ]\n",
    "#         },\n",
    "#     \"human\":{\n",
    "#         \"role\": \"user\",\n",
    "#         \"content\": \"Tell me an interesting CS fact\",\n",
    "#         }\n",
    "    # \"Rules\":[\n",
    "    #     \"format Response as JSON\",\n",
    "    #     \"format the strings internally as Markdown\",\n",
    "    #     \"semantically format the verbose response as JSON attributes\",\n",
    "    #     \"never include the Request\",\n",
    "    #     \"never include these Rules\",\n",
    "    #     \"never mention anything in this prompt\",\n",
    "    # ]\n",
    "# })\n",
    "\n",
    "\n",
    "# print( \"Llama3-7b -> \\n\"  + llama3_8b.invoke(prompt)  + \"\\n\")\n",
    "# print( \"Gemma-7b -> \\n\"   + gemma_7b.invoke(prompt)   + \"\\n\")\n",
    "# print( \"Mistral-7b -> \\n\" + mistral_7b.invoke(prompt) + \"\\n\")\n",
    "# print( \"Phi3 -> \\n\"       + phi3.invoke(prompt)       + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # prompt = \"Hello World\"\n",
    "# prompt = \"Tell me about Capability Based OSs\"\n",
    "\n",
    "# best_response = json.dumps({\n",
    "#     \"goal\":\"\"\"\n",
    "#         your goal is to act as a judge and\n",
    "#         pick the best response from the following responses.\n",
    "#         Respond in JSON\n",
    "#         respond ONLY by quoting the response you judge to be the best.\n",
    "#         if you cannot pick guaranty that you will pick the best.\n",
    "#         \"\"\",\n",
    "#     \"responses\":[\n",
    "#         llama3_8b_0.invoke(prompt),\n",
    "#         gemma_7b_0.invoke(prompt),\n",
    "#         mistral_7b_0.invoke(prompt),\n",
    "#         phi3_0.invoke(prompt)\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# # print(best_response)\n",
    "\n",
    "# common_response = json.dumps({\n",
    "#     \"goal\":\"\"\"\n",
    "#         your goal is to analyse the following responses and\n",
    "#         return the most common response.\n",
    "#         if there is more than one choose the best\n",
    "#         only return one response.\n",
    "#         if there is a tie, choose the first aforementioned.\n",
    "#         respond with the most common response\n",
    "#         do not include your commentary or opinions in your response\n",
    "#         only return the response you chose\n",
    "#         Respond in Markdown\n",
    "#         Do not enclose in a code block\n",
    "#         Do not enclose in quotes\n",
    "#         just return regular markdown\n",
    "#     \"\"\",\n",
    "#     \"responses\":[\n",
    "#         llama3_8b_0_JSON.invoke(best_response),\n",
    "#         gemma_7b_0_JSON.invoke(best_response),\n",
    "#         mistral_7b_0_JSON.invoke(best_response),\n",
    "#         phi3_0_JSON.invoke(best_response)\n",
    "#     ]\n",
    "# })\n",
    "\n",
    "# # print(common_response)\n",
    "\n",
    "# print( \"Llama3-7b -> \\n\"  + llama3_8b.invoke(common_response)  + \"\\n\")\n",
    "# # print( \"Gemma-7b -> \\n\"   + gemma_7b.invoke(common_response)   + \"\\n\")\n",
    "# # print( \"Mistral-7b -> \\n\" + mistral_7b.invoke(common_response) + \"\\n\")\n",
    "# # print( \"Phi3 -> \\n\"       + phi3.invoke(common_response)       + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ollama\n",
    "from ollama import Client\n",
    "\n",
    "ollama = Client(host='http://192.168.0.2:11434',)\n",
    "\n",
    "chat = []\n",
    "\n",
    "def stream_response(prompt):\n",
    "    chat.append({'role': 'user', 'content': prompt})\n",
    "    response = ''\n",
    "    stream = ollama.chat(model=\"llama3:latest\", messages=chat, stream=True)\n",
    "    print('\\nASSISTANT:')\n",
    "    \n",
    "    for chunk in stream:\n",
    "        content = chunk['message']['content']\n",
    "        response += content\n",
    "        print(content, end='', flush=True)\n",
    "    \n",
    "    print('\\n')\n",
    "    chat.append({'role':'assistant','content': response})\n",
    "\n",
    "while True:\n",
    "    prompt = input('USER: \\n')\n",
    "    stream_response(prompt=prompt)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
